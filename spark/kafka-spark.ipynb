{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['JAVA_HOME'] = '~/Data-Engineering_text-to-speech_data-collection/env/lib/python3.8/site-packages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up java the environment variable\n",
    "os.environ['JAVA_HOME'] ='/usr/lib/jvm/java-11-openjdk-amd64'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark version: 3.3.0\n",
      "findspark version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "print(f'pyspark version: {pyspark.__version__}')\n",
    "print(f'findspark version: {findspark.__version__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing lib\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import Catalog\n",
    "import pyspark.sql.functions as psf\n",
    "from pyspark.sql.functions import lit, col,sum,avg,max,first,min,mean\n",
    "import pyspark.ml as Pipeline\n",
    "import pyspark.ml.pipeline as pmpip\n",
    "import pyspark.ml.param as pmparam\n",
    "from pyspark.ml.param.shared import HasInputCol,HasOutputCol\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "from pyspark.sql.functions import struct,array,lit\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7faec23cc370>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating configuration\n",
    "spark_conf = SparkConf()\n",
    "spark_conf.setAll([\n",
    "    ('spark.master', 'local'),\n",
    "    ('spark.app.name', 'text-speech-model'),\n",
    "    ('spark.submit.deployMode', 'client'),\n",
    "    ('spark.ui.showConsoleProgress', 'true'),\n",
    "    ('spark.eventLog.enabled', 'false'),\n",
    "    ('spark.logConf', 'false'),\n",
    "    ('diriver_port', '8887'),\n",
    "    ('cores_max','4'),\n",
    "    ('ui_port','4040'),\n",
    "    ('spark.driver.host', 'localhost'),\n",
    "    ('spark.driver.memory','4g')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: diriver_port\n",
      "Warning: Ignoring non-Spark config property: cores_max\n",
      "Warning: Ignoring non-Spark config property: ui_port\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 14:24:39 WARN Utils: Your hostname, ekubay-HP-Laptop-15-da0xxx resolves to a loopback address: 127.0.1.1; using 192.168.0.36 instead (on interface wlo1)\n",
      "22/10/08 14:24:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 14:24:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create connection \n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>text-speech-model</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7faec1b8b880>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the details \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#current path\n",
    "path = '../data/Amharic News Dataset.csv' #g1-raw-text-data-topic-dev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# reading the data \n",
    "df_pyspark = spark.read.csv(path,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# here we can display texts\n",
    "random_row_text = df_pyspark.rdd.takeSample(False, 2, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(headline='የግድቡን መገንባት ስትደግፍ የነበረችው ሱዳን ኢትዮጵያ በመጭው ሀምሌ ወር ያቀደችውን የግድቡን የመጀመሪያ የውሃ ሙሌት አልደግፍም የሚል አቋም አንጸባርቃለች፡፡ ግብጽ በአንጻሩ ከግድቡ የመሰረት ድንጋይ መጣል ጀምሮ የሚደርሰኝ የውሃ መጠን ይቀንሳል በማለት ተቃውሞዋን እያሰማች ቆይታለች፤ የግድቡን የውሃ ሙሌቱ እቅድንም አትቀበለውም፡፡', category=None, date=None, views=None, article=None, link=None),\n",
       " Row(headline='ድርጅቱ ከባንኮች ጋር መሥራቱ አስተማማኝ የክፍያ ሥርዓት እንዲፈጠር አስችሏል', category='ቢዝነስ', date='June 21, 2017', views='Unknown', article='የኢትዮጵያ የምርት ገበያ ድርጅት ከባንኮች ጋር\\xa0 በጋራ በመስራቱ በተገበያዮች ላይ የክፍያ መተማመን እንዲፈጠር ማስቻሉን አስታውቋል፡፡ምርት\\xa0 ገበያው ከአሁን ቀደም ከአስር ባንኮች ጋር በጋር በመስራቱ ላለፉት ዘጠኝ ዓመታት\\xa0 በሚያከናውነው የግብይት ሂደት ላይ \\xa0መተማመንን \\xa0የፈጠረ የክፍያ ሥርዓት እንዲኖር አስችሏል ብሏል፡፡በዛሬው ዕለት ምርት ገበያው ከብርሃን ኢንተርናሽናል ባንክ ጋር በጋር\\xa0 ለመስራት የሚያስችለውን ስምምነት ተፈራርሟል፡፡የአሁኑን ስምምነት ተከትሎ ምርት ገበያው ደንበኞቹ በአስራ አንድ ባንኮች የክፍያ አገልግሎት መፈጸም\\xa0 እንዲችሉ የሚያደርግ ነው፡፡በአሁኑ ወቅት ድርጅቱ በዘጠኝ የሀገሪቱ አካባቢዎች እየገነባቸው ባሉ የግብይት መፈፀሚያ ማዕከላት ሻጭና አቅራቢዎች የክፍያ አገልግሎት እንዲፈጽሙ የሚያስችል ነው ብለዋል ዋና ስራ አስፈፃሚው አቶ ኤርሚያስ እሸቱ፡፡\\xa0', link='https://waltainfo.com/am/23211/')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying the raw data\n",
    "random_row_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----------------+-----+--------------------+--------------------+\n",
      "|            headline|category|             date|views|             article|                link|\n",
      "+--------------------+--------+-----------------+-----+--------------------+--------------------+\n",
      "|የኦሊምፒክ ማጣሪያ ተሳታፊዎ...|    ስፖርት| January 14, 2021|    2|ብርሃን ፈይሳየኢትዮጵያ ቦክ...|https://www.press...|\n",
      "|          አዲስ ዘመን ድሮ|    መዝናኛ|December 28, 2020|    4| የአዲስ ዘመን ጋዜጣ ቀደም...|                null|\n",
      "|      መንገድ በመከተል አልፎ|    null|             null| null|                null|                null|\n",
      "|      አልፎ በሚገኙት ሰፈሮች|    null|             null| null|                null|                null|\n",
      "|       ብዙዎች የልኳንዳ ሥጋ|    null|             null| null|                null|                null|\n",
      "+--------------------+--------+-----------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark dataframe\n",
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Streaming Text**\n",
    "\n",
    "Now after generating the random text, then let the streaming begin and we can begin to immediately stream the data into the frontend for the user to see. For this service we shall use apache kafka. And we shall send the publish the random text data, keeping in mind that we had uniquely generated the uuid and we configure this with the api, therefore there shall be no overlaps with the data, so once we have received it. We consume the text and send back an audio file that we are going to carefully but keenly scrutinize before funishing it to the data lake that is our s3 bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Using cached kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n"
     ]
    }
   ],
   "source": [
    "# streaming text\n",
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming text\n",
    "from kafka import KafkaConsumer,KafkaProducer,KafkaAdminClient\n",
    "from kafka.admin import NewTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating id \n",
    "import uuid\n",
    "\n",
    "text = {\n",
    "    \"id\":str(uuid.uuid4())\n",
    "}\n",
    "#A universally unique identifier (UUid) is a 128-bit label used for information in computer systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=bootstrap-0 host=127.0.0.1:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 111. Disconnecting.\n"
     ]
    },
    {
     "ename": "NoBrokersAvailable",
     "evalue": "NoBrokersAvailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#creating clients\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m client \u001b[39m=\u001b[39m KafkaAdminClient(bootstrap_servers\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m127.0.0.1:9092\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m                          client_id\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madmin-client\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/admin/client.py:208\u001b[0m, in \u001b[0;36mKafkaAdminClient.__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    205\u001b[0m reporters \u001b[39m=\u001b[39m [reporter() \u001b[39mfor\u001b[39;00m reporter \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mmetric_reporters\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m    206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metrics \u001b[39m=\u001b[39m Metrics(metric_config, reporters)\n\u001b[0;32m--> 208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client \u001b[39m=\u001b[39m KafkaClient(metrics\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_metrics,\n\u001b[1;32m    209\u001b[0m                            metric_group_prefix\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39madmin\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    210\u001b[0m                            \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig)\n\u001b[1;32m    211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mcheck_version(timeout\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mapi_version_auto_timeout_ms\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m/\u001b[39m \u001b[39m1000\u001b[39m))\n\u001b[1;32m    213\u001b[0m \u001b[39m# Get auto-discovered version from client if necessary\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/client_async.py:244\u001b[0m, in \u001b[0;36mKafkaClient.__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mapi_version\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     check_timeout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mapi_version_auto_timeout_ms\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m/\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m--> 244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mapi_version\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_version(timeout\u001b[39m=\u001b[39;49mcheck_timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/client_async.py:900\u001b[0m, in \u001b[0;36mKafkaClient.check_version\u001b[0;34m(self, node_id, timeout, strict)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39mif\u001b[39;00m try_node \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m--> 900\u001b[0m     \u001b[39mraise\u001b[39;00m Errors\u001b[39m.\u001b[39mNoBrokersAvailable()\n\u001b[1;32m    901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_connect(try_node)\n\u001b[1;32m    902\u001b[0m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conns[try_node]\n",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m: NoBrokersAvailable"
     ]
    }
   ],
   "source": [
    "#creating clients\n",
    "client = KafkaAdminClient(bootstrap_servers=['127.0.0.1:9092'],\n",
    "                         client_id='admin-client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text update\n",
    "text.update({'headline':random_row_text[0]['headline']})\n",
    "text.update({'article':random_row_text[0]['article']})\n",
    "text.update({'audio':'../data/test_amharic.wav'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating topic\n",
    "topic_list = []\n",
    "topic_list.append(\n",
    "    NewTopic(name='spark-transformed-text',\n",
    "              num_partitions=1,\n",
    "              replication_factor=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b6ee45c0530d00bd9ce3b24e915dd77806354f7ba58bdfbe06eaf4782981504"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
